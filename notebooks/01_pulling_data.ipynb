{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Energy Forecasting\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project focuses on analyzing historical solar irradiance and other features to train a model. With this model using weather forecasts one should be able to predict solar energy generation.\n",
    "This data is retrieved from the [Photovoltaic Geographical Information System (PVGIS)](https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/pvgis-tools/hourly-radiation_en).\n",
    "\n",
    "### Data Source (Historical solar radiation data)\n",
    "The dataset is fetched using the PVGIS API and includes parameters:\n",
    "- **Global in-plane irradiance (G(i))**: Measured in W\\m2\n",
    "- **Air temperature (T2m)**: Measured in °C\n",
    "- **Wind speed (WS10m)**: Measured in m/s at 10m\n",
    "- **Location**: Latitude and Longitude chosen for Freiburg im Breisgau\n",
    "- **Time**: in UTC. Span of time for training data is the year 2020 with hourly data, i.e. 8760 entries, each entry representing 1 hour of the year (possible data to fetch from this homepage for years: 2005-2020)\n",
    "\n",
    "### Project Goal\n",
    "The objective is to preprocess, analyze, and build models that can predict solar energy generation for Baden-Württemberg based on weather forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, UTC\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_historical_weather_data(save=False, save_path=None, start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    Pulls historical weather data. Years of data range from 2005 to 2020.\n",
    "    Latitude and Longitude of Freiburg im Breisgau is being used.\n",
    "    Documentation: https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/getting-started-pvgis/api-non-interactive-service_en\n",
    "    \"\"\"\n",
    "\n",
    "    # Latitude and Longitude of Freiburg im Breisgau\n",
    "    lat = \"47.99\"\n",
    "    lon = \"7.84\"\n",
    "    # API URL\n",
    "    url = f\"https://re.jrc.ec.europa.eu/api/v5_2/seriescalc?lat={lat}&lon={lon}&startyear={str(start_year)}&endyear={str(end_year)}&outputformat=csv\"\n",
    "\n",
    "    # Fetch data\n",
    "    response = requests.get(url)\n",
    "    print(\"response code: \", response.status_code)\n",
    "\n",
    "    # Check if the response is valid\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # Split the response into lines\n",
    "            lines = response.text.split(\"\\n\")\n",
    "\n",
    "            # Goal here: Get the column names and the actual data\n",
    "            column_names = []\n",
    "            data_lines = []\n",
    "\n",
    "            # Loop over lines, if a line starts with \"time\" use the entries of that line for the column names\n",
    "            for line in lines:\n",
    "                if line.startswith(\"time\"):  # Header row found\n",
    "                    column_names = line.strip().split(\",\")\n",
    "                    continue  # Skip adding the header to data_lines\n",
    "                if column_names and not any(c.isalpha() for c in line.split(\",\")[:]):  \n",
    "                    # Only keep rows that don’t contain letters\n",
    "                    data_lines.append(line)\n",
    "\n",
    "            if column_names is None:\n",
    "                raise ValueError(\"Header not found in response data.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error while parsing CSV:\", str(e))\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "    print(\"Extracted column_names: \", column_names)\n",
    "\n",
    "    # get data ready to be moved into a df by joining the data\n",
    "    csv_data = \"\\n\".join(data_lines)\n",
    "    df = pd.read_csv(StringIO(csv_data), names=column_names, header=None)\n",
    "\n",
    "    # Remove footer metadata: Keep only rows where \"time\" is a valid timestamp\n",
    "    df = df[df[\"time\"].str.match(r\"^\\d{8}:\\d{4}$\", na=False)]\n",
    "\n",
    "    # use datetime format for time column\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", format=\"%Y%m%d:%H%M\")\n",
    "    # use float64 format for G(i) column\n",
    "    df[\"G(i)\"] = pd.to_numeric(df[\"G(i)\"], errors=\"coerce\")\n",
    "\n",
    "    # Get some more information on the data\n",
    "    #print(\"DFDFDF\", df)\n",
    "    #print(\"dtypes\\n\", df.dtypes)\n",
    "    #print(\"df.describe\", df.describe())\n",
    "    #print(\"NaNs\\n\", df.isna().sum())\n",
    "\n",
    "    # drop unwanted columns\n",
    "    df.drop(columns=[\"H_sun\", \"Int\"], inplace=True)\n",
    "\n",
    "    if save == True:\n",
    "        # Save dataframe as pickle\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            pickle.dump(df, file)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_milliseconds(date_string):\n",
    "    \"\"\"\n",
    "    Helper function to convert custom date (YYYY-MM-DD HH:MM:SS) to Unix timestamp in milliseconds\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(dt.timestamp() * 1000)\n",
    "\n",
    "\n",
    "def get_available_timestamps(filter, region, resolution):\n",
    "    \"\"\"\n",
    "    Fetches ALL available timestamps, which we will filter and then use to fetch the actual data\n",
    "    \"\"\"\n",
    "    url = f\"https://www.smard.de/app/chart_Data/{filter}/{region}/index_{resolution}.json\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check for status code and get a list of the timestamps if status code is fine\n",
    "    if response.status_code == 200:\n",
    "        timestamps = response.json().get(\"timestamps\", [])\n",
    "        return timestamps\n",
    "    else:\n",
    "        print(f\"Failed to retrieve timestamps: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def pull_historical_energy_data(save=False, save_path=None, start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    In the SMARD API each timestamp corresponds to the start of an available dataset.\n",
    "    Thus a single timestamp does not fetch a full time range — we need to collect data for multiple timestamps to cover a period.\n",
    "\n",
    "    So, to get the energy data from this API we first need to get certain timestamps through a request.\n",
    "    Once we have those timestamps we can use them in another request to get the actual data behind those timestamps.\n",
    "    Using the region 'TransnetBW', which spans over the state of Baden-Württemberg - the state where Freiburg im Breisgau is located.\n",
    "    Energy in MWh.\n",
    "    Documentation: https://github.com/bundesAPI/smard-api\n",
    "    \"\"\"\n",
    "\n",
    "    # Using a slightly expanded interval of time to look for timestamps that include the data we want.\n",
    "    # Expanding by using roughly 1 past week before start_year and roughly 1 week after end_year to make\n",
    "    # sure to get the entire interval of time we are interested in\n",
    "    start_date = f\"{start_year - 1}-12-24 00:00:00\"\n",
    "    end_date = f\"{end_year + 1}-01-08 00:00:00\"\n",
    "\n",
    "    # API call: parameters\n",
    "    filter = \"4068\" # code for energy generated by photovoltaics\n",
    "    filterCopy = filter # must be specified according to the documentation\n",
    "    region = \"TransnetBW\" # TSO for Baden-Württemberg (state) in Germany\n",
    "    regionCopy = region # must be specified according to the documentation\n",
    "    resolution = \"hour\" # hourly resolution of the data\n",
    "    \n",
    "    # Convert input dates to timestamps, because we need timestamps in milliseconds for the API to get all available timestamps\n",
    "    start_ts = date_to_milliseconds(start_date)\n",
    "    end_ts = date_to_milliseconds(end_date)\n",
    "    print(f\"Fetching data from {start_date} ({start_ts}) to {end_date} ({end_ts})\")\n",
    "    print(f\"Which corresponds to {(end_ts - start_ts)/(3600*1000)} hours.\")\n",
    "\n",
    "    # Get all available timestamps\n",
    "    available_timestamps = get_available_timestamps(filter=filter, region=region, resolution=resolution)\n",
    "    if not available_timestamps:\n",
    "        print(\"Problem with retrieving available timestamps.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter all the pulled available timestamps to get only those matching our time interval of interest\n",
    "    selected_timestamps = [ts for ts in available_timestamps if start_ts <= ts <= end_ts]\n",
    "    if not selected_timestamps:\n",
    "        print(\"No available timestamps in the given range.\")\n",
    "\n",
    "    print(f\"Fetching data for {len(selected_timestamps)} timestamps.\")\n",
    "\n",
    "    # Finally fetch the actual data using the pulled and filtered timestamps\n",
    "    all_data = []\n",
    "    for timestamp in selected_timestamps:\n",
    "        url = f\"https://www.smard.de/app/chart_data/{filter}/{region}/{filterCopy}_{regionCopy}_{resolution}_{timestamp}.json\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_data.append(data)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for timestamp {timestamp}\")\n",
    "\n",
    "    # Extract the time-series data\n",
    "    data_list = []\n",
    "    for entry in all_data:\n",
    "        metadata = entry.get(\"smeta_data\", {})\n",
    "        series = entry.get(\"series\", [])\n",
    "\n",
    "        for timestamp, value in series:\n",
    "            utc_date = datetime.fromtimestamp(timestamp / 1000, UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            data_list.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"datetime\": utc_date,\n",
    "                \"value\": value,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    first_timestamp = df[\"datetime\"].iloc[0]\n",
    "    last_timestamp = df[\"datetime\"].iloc[-1]\n",
    "\n",
    "    print(f\"Given the start_date {start_date} and end_date {end_date} the available timestamps are ranging from {first_timestamp} to {last_timestamp}\")\n",
    "\n",
    "    # The returned df (and all_data) includes data for the time interval of interest, but it also includes data outside of that time interval of interest\n",
    "    # Matching the data properly is done in the next step in the pipeline with functions from data_preprocessing.py\n",
    "    return df, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response code:  200\n",
      "Extracted column_names:  ['time', 'G(i)', 'H_sun', 'T2m', 'WS10m', 'Int']\n",
      "                 time  G(i)   T2m  WS10m\n",
      "0 2017-01-01 00:10:00   0.0 -2.92   1.59\n",
      "1 2017-01-01 01:10:00   0.0 -2.95   1.52\n",
      "2 2017-01-01 02:10:00   0.0 -2.78   1.59\n",
      "3 2017-01-01 03:10:00   0.0 -2.85   1.66\n",
      "4 2017-01-01 04:10:00   0.0 -2.91   1.66\n",
      "                     time  G(i)   T2m  WS10m\n",
      "26275 2019-12-31 19:10:00   0.0 -1.59   1.93\n",
      "26276 2019-12-31 20:10:00   0.0 -1.90   1.93\n",
      "26277 2019-12-31 21:10:00   0.0 -1.95   2.00\n",
      "26278 2019-12-31 22:10:00   0.0 -1.60   2.21\n",
      "26279 2019-12-31 23:10:00   0.0 -1.09   2.28\n",
      "time     datetime64[ns]\n",
      "G(i)            float64\n",
      "T2m             float64\n",
      "WS10m           float64\n",
      "dtype: object\n",
      "                      time          G(i)           T2m         WS10m\n",
      "count                26280  26280.000000  26280.000000  26280.000000\n",
      "mean   2018-07-02 11:40:00    138.302451      8.883917      1.911023\n",
      "min    2017-01-01 00:10:00      0.000000    -19.780000      0.000000\n",
      "25%    2017-10-01 17:55:00      0.000000      2.400000      1.240000\n",
      "50%    2018-07-02 11:40:00      0.000000      8.610000      1.720000\n",
      "75%    2019-04-02 05:25:00    191.000000     15.182500      2.280000\n",
      "max    2019-12-31 23:10:00    968.010000     33.030000      9.720000\n",
      "std                    NaN    222.469028      8.265733      1.076786\n"
     ]
    }
   ],
   "source": [
    "hist_weather_data = pull_historical_weather_data(save=False, save_path=None, start_year=2017, end_year=2019)\n",
    "print(hist_weather_data.head())\n",
    "print(hist_weather_data.tail())\n",
    "print(hist_weather_data.dtypes)\n",
    "print(hist_weather_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from 2016-12-24 00:00:00 (1482534000000) to 2020-01-08 00:00:00 (1578438000000)\n",
      "Which corresponds to 26640.0 hours.\n",
      "Fetching data for 159 timestamps.\n",
      "Given the start_date 2016-12-24 00:00:00 and end_date 2020-01-08 00:00:00 the available timestamps are ranging from 2016-12-25 23:00:00 to 2020-01-12 22:00:00\n",
      "       timestamp             datetime  value\n",
      "0  1482706800000  2016-12-25 23:00:00    0.0\n",
      "1  1482710400000  2016-12-26 00:00:00    0.0\n",
      "2  1482714000000  2016-12-26 01:00:00    0.0\n",
      "3  1482717600000  2016-12-26 02:00:00    0.0\n",
      "4  1482721200000  2016-12-26 03:00:00    0.0\n",
      "           timestamp             datetime  value\n",
      "26707  1578852000000  2020-01-12 18:00:00    0.0\n",
      "26708  1578855600000  2020-01-12 19:00:00    0.0\n",
      "26709  1578859200000  2020-01-12 20:00:00    0.0\n",
      "26710  1578862800000  2020-01-12 21:00:00    0.0\n",
      "26711  1578866400000  2020-01-12 22:00:00    0.0\n",
      "timestamp      int64\n",
      "datetime      object\n",
      "value        float64\n",
      "dtype: object\n",
      "          timestamp         value\n",
      "count  2.671200e+04  26712.000000\n",
      "mean   1.530787e+12    607.314531\n",
      "std    2.776044e+10    953.719839\n",
      "min    1.482707e+12      0.000000\n",
      "25%    1.506747e+12      0.000000\n",
      "50%    1.530787e+12     10.500000\n",
      "75%    1.554826e+12    916.000000\n",
      "max    1.578866e+12   4044.500000\n"
     ]
    }
   ],
   "source": [
    "hist_energy_data, _ = pull_historical_energy_data(save=False, save_path=None, start_year=2017, end_year=2019)\n",
    "print(hist_energy_data.head())\n",
    "print(hist_energy_data.tail())\n",
    "print(hist_energy_data.dtypes)\n",
    "print(hist_energy_data.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
